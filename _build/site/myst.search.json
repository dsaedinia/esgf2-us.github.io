{"version":"1","records":[{"hierarchy":{"lvl1":"CMIP Cheatsheet"},"type":"lvl1","url":"/cmip-cheatsheet","position":0},{"hierarchy":{"lvl1":"CMIP Cheatsheet"},"content":"","type":"content","url":"/cmip-cheatsheet","position":1},{"hierarchy":{"lvl1":"CMIP Cheatsheet","lvl2":"Introduction"},"type":"lvl2","url":"/cmip-cheatsheet#introduction","position":2},{"hierarchy":{"lvl1":"CMIP Cheatsheet","lvl2":"Introduction"},"content":"The Coupled Model Intercomparison Project (CMIP) is an international climate modelling project, designed to better understand past, present and future changes in the climate.\n\nA climate model is a complex computer code that creates a digital analogue to Earth. This model digitises the processes and interactions between parts of Earth‚Äôs climate system: the atmosphere, ocean, land surface, cryosphere and biosphere. We use models to experiment how future changes in human activities will impact the Earth‚Äôs future climate, how much it warms, how floods, droughts and other extremes will change.\n\nHowever, many processes in our climate occur on such small scales, that models are not able to exactly represent them in models, and therefore some simplifications are required. How we simplify the climate system is unique to each model. Therefore comparing simulations from different models is useful for understanding which results are consistent across models, and which results are less agreed upon. Since 1995, CMIP has been coordinating this model intercomparison across the climate science community.\n\nThis multi-model approach helps to evaluate climate models, leads to improvements in the model simulations and provides a better understanding of past, present and future climates. One additional strength of CMIP lies in its global infrastructure which has gathered the data and gives open access for a growing global research community.\n\nCMIP has grown from a modest scientific research initiative in the early nineties to become a global enterprise: more than 50 modelling centres around the world are participating in the sixth phase of CMIP, CMIP6. Many hundreds of scientific papers have already been published and the results are taken into account for policy decisions.\n\nCMIP has been organised in different phases, each with new and improved climate model experiment protocols, standards, and data distribution mechanisms. CMIP6 is the most recent phase to release its modelling output data for general use, whilst the latest phase, CMIP7 is in its earliest organisational stages.\n\nVisit \n\nWCRP-CMIP for more information.","type":"content","url":"/cmip-cheatsheet#introduction","position":3},{"hierarchy":{"lvl1":"CMIP Cheatsheet","lvl2":"Helpful Links:"},"type":"lvl2","url":"/cmip-cheatsheet#helpful-links","position":4},{"hierarchy":{"lvl1":"CMIP Cheatsheet","lvl2":"Helpful Links:"},"content":"CMIP Phases: \n\nWCRP-CMIP Phases\n\nWCRP-CMIP Github: \n\nWCRP-CMIP\n\nCMIP Data Access: \n\nWCRP-CMIP Data Access","type":"content","url":"/cmip-cheatsheet#helpful-links","position":5},{"hierarchy":{"lvl1":"ESGF2-US Cookbook"},"type":"lvl1","url":"/cookbooks","position":0},{"hierarchy":{"lvl1":"ESGF2-US Cookbook"},"content":"","type":"content","url":"/cookbooks","position":1},{"hierarchy":{"lvl1":"ESGF2-US Cookbook","lvl2":"Overview"},"type":"lvl2","url":"/cookbooks#overview","position":2},{"hierarchy":{"lvl1":"ESGF2-US Cookbook","lvl2":"Overview"},"content":"A cookbook is essentially a ‚Äúrecipe book‚Äù of reusable, domain‚Äëspecific workflows implemented as Jupyter Notebooks. This cookbook focuses on highlighting analysis recipes, as well as data acccess methods, all accesible within the Python programming language.","type":"content","url":"/cookbooks#overview","position":3},{"hierarchy":{"lvl1":"ESGF2-US Cookbook","lvl2":"Helpful Link:"},"type":"lvl2","url":"/cookbooks#helpful-link","position":4},{"hierarchy":{"lvl1":"ESGF2-US Cookbook","lvl2":"Helpful Link:"},"content":"ESGF-cookbook","type":"content","url":"/cookbooks#helpful-link","position":5},{"hierarchy":{"lvl1":"Debuggers Guide"},"type":"lvl1","url":"/debugging","position":0},{"hierarchy":{"lvl1":"Debuggers Guide"},"content":"","type":"content","url":"/debugging","position":1},{"hierarchy":{"lvl1":"Debuggers Guide"},"type":"lvl1","url":"/debugging#debuggers-guide","position":2},{"hierarchy":{"lvl1":"Debuggers Guide"},"content":"","type":"content","url":"/debugging#debuggers-guide","position":3},{"hierarchy":{"lvl1":"Developers Guide"},"type":"lvl1","url":"/developer-guide","position":0},{"hierarchy":{"lvl1":"Developers Guide"},"content":"","type":"content","url":"/developer-guide","position":1},{"hierarchy":{"lvl1":"Developers Guide"},"type":"lvl1","url":"/developer-guide#developers-guide","position":2},{"hierarchy":{"lvl1":"Developers Guide"},"content":"","type":"content","url":"/developer-guide#developers-guide","position":3},{"hierarchy":{"lvl1":"ESG Publisher"},"type":"lvl1","url":"/esg-publisher","position":0},{"hierarchy":{"lvl1":"ESG Publisher"},"content":"","type":"content","url":"/esg-publisher","position":1},{"hierarchy":{"lvl1":"ESG Publisher","lvl2":"Publisher Introduction"},"type":"lvl2","url":"/esg-publisher#publisher-introduction","position":2},{"hierarchy":{"lvl1":"ESG Publisher","lvl2":"Publisher Introduction"},"content":"The esg-publisher or esgcet Python package contains a collection of command-line utilities to scan, manipulate and push dataset metadata to an ESGF index node.  The basic publication process includes several basic steps and sometimes optional steps. Publisher functionality is available via several submodles/classes in the package.\n\nThe publisher software has undergone a significant change starting with v5.* of the software.  Prior versions involved storage of dataset metadata in the legacy ESGF data node PostgreSQL database and generation of THREDDS catalogs.   The actual publication to the ESGF index occured via catalog harvesting.  Instead, the more recent publisher simplifies the process with the following phases:\n\n#. Local scan of datasets (featuring the autocurator package by default)\n#. Record generation using scan, mapfile and auxiliary (json) information/files as input\n#. Update check of existing dataset, previous version manipulation.\n#. Push/publish of record(s) to ESGF index\n\nAnd several optional project-specific phases:\n\nAutomatic metadata checking with PrePARE (CMIP6-only as of today)\n\nPID registration and citiation URL generation (CMIP6 and input4MIPs)\n\nFor those familiar with the previous publisher, please be aware of the following distinctions between earlier versions and v5.*\n\nA Python3 conda environment is required (most prior versions have run Python2)\n\nthe configuration (.ini) file format is new and have been vastly simplified.  Note that the old format for project-specific .ini files are still used by the esgf-prepare tools (eg. esgmapfile).  The v5. publisher has the ability to migrate the needed settings from the previous ini files.\n\nPrior invocation of esgpublish required use of --thredds and --publish stages.  Those arguments are eliminated.  In the general case, you can run esgpublish in a single command.  Advanced users may chose to run the individual publishing steps separately to create workflows, for instance, in the use of an external workflow manager.","type":"content","url":"/esg-publisher#publisher-introduction","position":3},{"hierarchy":{"lvl1":"ESG Publisher","lvl2":"Prerequisites"},"type":"lvl2","url":"/esg-publisher#prerequisites","position":4},{"hierarchy":{"lvl1":"ESG Publisher","lvl2":"Prerequisites"},"content":"conda eg. Miniconda <https://docs.conda.io/en/latest/miniconda.html>_  installation.\n\nMountpoint to located data on the same host as publisher software installation, so the publisher scan utility (eg. autocurator) has access.\n\nBasic dataset information provided via the esg mapfile format.   The most popular approach is using the esgf-prepare/esgmapfile <https://esgf.github.io/esgf-prepare/>_ utility.","type":"content","url":"/esg-publisher#prerequisites","position":5},{"hierarchy":{"lvl1":"ESG Publisher","lvl2":"Helpful Links:"},"type":"lvl2","url":"/esg-publisher#helpful-links","position":6},{"hierarchy":{"lvl1":"ESG Publisher","lvl2":"Helpful Links:"},"content":"ESG Publisher Documentation: \n\nESG Publisher Intro\n\nGithub: \n\nESG Publisher Github","type":"content","url":"/esg-publisher#helpful-links","position":7},{"hierarchy":{"lvl1":"ESG Pull"},"type":"lvl1","url":"/esg-pull","position":0},{"hierarchy":{"lvl1":"ESG Pull"},"content":"","type":"content","url":"/esg-pull","position":1},{"hierarchy":{"lvl1":"ESG Pull","lvl2":"Introduction"},"type":"lvl2","url":"/esg-pull#introduction","position":2},{"hierarchy":{"lvl1":"ESG Pull","lvl2":"Introduction"},"content":"esgpull is a modern ESGF data management tool, bundled with a custom asynchronous interface with the \n\nESGF Search API.\nIt handles scanning, downloading and updating datasets, files and queries from ESGF. Its simple data model makes `esgpull` easy to use, it is completely possible to never download a single file and still find a use for it. ","type":"content","url":"/esg-pull#introduction","position":3},{"hierarchy":{"lvl1":"ESG Pull","lvl3":"Feature highlight","lvl2":"Introduction"},"type":"lvl3","url":"/esg-pull#feature-highlight","position":4},{"hierarchy":{"lvl1":"ESG Pull","lvl3":"Feature highlight","lvl2":"Introduction"},"content":"Simple syntax for fast data exploration\n\nAsynchronous download\n\nHighly configurable\n\nPlugin system for extensibility","type":"content","url":"/esg-pull#feature-highlight","position":5},{"hierarchy":{"lvl1":"ESG Pull","lvl3":"Setup","lvl2":"Introduction"},"type":"lvl3","url":"/esg-pull#setup","position":6},{"hierarchy":{"lvl1":"ESG Pull","lvl3":"Setup","lvl2":"Introduction"},"content":"Run $ pip install esgpull to install the latest release and its dependencies.\n\nHave a look at the \n\nInstallation page for more ways to install.","type":"content","url":"/esg-pull#setup","position":7},{"hierarchy":{"lvl1":"ESG Pull","lvl3":"Quickstart","lvl2":"Introduction"},"type":"lvl3","url":"/esg-pull#quickstart","position":8},{"hierarchy":{"lvl1":"ESG Pull","lvl3":"Quickstart","lvl2":"Introduction"},"content":"Jump directly to the \n\nQuickstart guide to get to know how to use esgpull. [ESGF portal]: https://esgf-node.ipsl.upmc.fr/search/cmip6-ipsl ","type":"content","url":"/esg-pull#quickstart","position":9},{"hierarchy":{"lvl1":"ESG Pull","lvl3":"Helpful Links:","lvl2":"Introduction"},"type":"lvl3","url":"/esg-pull#helpful-links","position":10},{"hierarchy":{"lvl1":"ESG Pull","lvl3":"Helpful Links:","lvl2":"Introduction"},"content":"ESG Pull Documentation: \n\nIntro to ESG Pull\n\nESG Pull Github: \n\nESG Pull Github","type":"content","url":"/esg-pull#helpful-links","position":11},{"hierarchy":{"lvl1":"ESG Voc"},"type":"lvl1","url":"/esg-voc","position":0},{"hierarchy":{"lvl1":"ESG Voc"},"content":"","type":"content","url":"/esg-voc","position":1},{"hierarchy":{"lvl1":"ESG Voc","lvl2":"Introduction"},"type":"lvl2","url":"/esg-voc#introduction","position":2},{"hierarchy":{"lvl1":"ESG Voc","lvl2":"Introduction"},"content":"ESGVOC is a Python library designed to streamline the management of controlled vocabularies (CV) used by the climate modelling community publishing datasets related to WCRP-ESMO (\n\nhttps://‚Äãwww‚Äã.wcrp‚Äã-esmo‚Äã.org) activities on the ESGF (\n\nhttps://‚Äãesgf‚Äã.llnl‚Äã.gov). By harmonizing vocabulary terms and providing both a Python API and a CLI for easy access, ESGVOC resolves common issues like inconsistencies, errors, and inefficiencies associated with managing controlled vocabularies.","type":"content","url":"/esg-voc#introduction","position":3},{"hierarchy":{"lvl1":"ESG Voc","lvl3":"Why ESGVOC?","lvl2":"Introduction"},"type":"lvl3","url":"/esg-voc#why-esgvoc","position":4},{"hierarchy":{"lvl1":"ESG Voc","lvl3":"Why ESGVOC?","lvl2":"Introduction"},"content":"Previously, controlled vocabularies were stored in multiple locations and formats, requiring various software implementations to query and interpret data. This approach introduced challenges, including:\n\nErrors and inconsistencies across systems.\n\nMisuse of metadata and data.\n\nDifficulty in maintaining and updating vocabularies.\n\nESGVOC improves controlled vocabulary management through two main ideas:\n\nHarmonization terms through a unified CV sourceA single, centralized repository ‚Äî referred to as the ‚ÄúUniverse CV‚Äù ‚Äî hosts all controlled vocabularies. Specialized vocabularies for specific projects reference the Universe CV via streamlined lists of IDs. This ensures consistency and eliminates duplication.\n\nProviding both pythin api and a CLIESGVOC provides a dedicated service for interacting with controlled vocabularies. It enables developers, administrators, and software systems to access vocabularies seamlessly via:\n\nA Python API for programmatic interaction.\n\nA CLI powered by \n\nTyper for command-line use.","type":"content","url":"/esg-voc#why-esgvoc","position":5},{"hierarchy":{"lvl1":"ESG Voc","lvl3":"Installation","lvl2":"Introduction"},"type":"lvl3","url":"/esg-voc#installation","position":6},{"hierarchy":{"lvl1":"ESG Voc","lvl3":"Installation","lvl2":"Introduction"},"content":"You can install ESGVOC using recent Python packaging tools. It is only available in \n\npypi.org (not in \n\nanaconda.org). We recommend the following methods:","type":"content","url":"/esg-voc#installation","position":7},{"hierarchy":{"lvl1":"ESG Voc","lvl4":"Using UV (preferred)","lvl3":"Installation","lvl2":"Introduction"},"type":"lvl4","url":"/esg-voc#using-uv-preferred","position":8},{"hierarchy":{"lvl1":"ESG Voc","lvl4":"Using UV (preferred)","lvl3":"Installation","lvl2":"Introduction"},"content":"UV is recommended for managing dependencies and isolating the library:uv add esgvoc\n\nThis ensures all dependencies are installed, and cached repositories and databases will be stored in the .cache directory alongside the .venv folder. This approach simplifies updates and uninstallation.","type":"content","url":"/esg-voc#using-uv-preferred","position":9},{"hierarchy":{"lvl1":"ESG Voc","lvl4":"Using pip in a virtual environment","lvl3":"Installation","lvl2":"Introduction"},"type":"lvl4","url":"/esg-voc#using-pip-in-a-virtual-environment","position":10},{"hierarchy":{"lvl1":"ESG Voc","lvl4":"Using pip in a virtual environment","lvl3":"Installation","lvl2":"Introduction"},"content":"Alternatively, you can use a virtual environment:python -m venv myenv\nsource myenv/bin/activate\npip install esgvoc","type":"content","url":"/esg-voc#using-pip-in-a-virtual-environment","position":11},{"hierarchy":{"lvl1":"ESG Voc","lvl3":"Fetching vocabulary data","lvl2":"Introduction"},"type":"lvl3","url":"/esg-voc#fetching-vocabulary-data","position":12},{"hierarchy":{"lvl1":"ESG Voc","lvl3":"Fetching vocabulary data","lvl2":"Introduction"},"content":"Once installed esgvoc need to clone the following WCRP CV repositories and cache them into an SQLite database:\n\nESGVOC primarily uses the following repositories for controlled vocabulary data:\n\nUniverse CV: \n\nGitHub Repository\n\nCMIP6 CVs: \n\nGitHub Repository\n\nCMIP6Plus CVs: \n\nGitHub Repository.. warning::\n   To be accurate, ESGVOC uses the specific branch \"esgvoc\" in those repositories.\n\nthose are configured by default !esgvoc install\n\nThis command performs the following actions:\n\nClones the official repositories.\n\nBuilds a cached SQLite database from the cloned data.","type":"content","url":"/esg-voc#fetching-vocabulary-data","position":13},{"hierarchy":{"lvl1":"ESG Voc","lvl4":"Offline use","lvl3":"Fetching vocabulary data","lvl2":"Introduction"},"type":"lvl4","url":"/esg-voc#offline-use","position":14},{"hierarchy":{"lvl1":"ESG Voc","lvl4":"Offline use","lvl3":"Fetching vocabulary data","lvl2":"Introduction"},"content":"If there is no internet access, it is still possible to use the library: copy the repositories into .cache/repos and then issue esgvoc install. The library will check the .cache/repos directory for existing repositories.","type":"content","url":"/esg-voc#offline-use","position":15},{"hierarchy":{"lvl1":"ESG Voc","lvl3":"Official controlled vocabulary repositories","lvl2":"Introduction"},"type":"lvl3","url":"/esg-voc#official-controlled-vocabulary-repositories","position":16},{"hierarchy":{"lvl1":"ESG Voc","lvl3":"Official controlled vocabulary repositories","lvl2":"Introduction"},"content":"","type":"content","url":"/esg-voc#official-controlled-vocabulary-repositories","position":17},{"hierarchy":{"lvl1":"ESG Voc","lvl4":"Flexibility for other repositories","lvl3":"Official controlled vocabulary repositories","lvl2":"Introduction"},"type":"lvl4","url":"/esg-voc#flexibility-for-other-repositories","position":18},{"hierarchy":{"lvl1":"ESG Voc","lvl4":"Flexibility for other repositories","lvl3":"Official controlled vocabulary repositories","lvl2":"Introduction"},"content":"While designed for these repositories, ESGVOC can use other repositories if they are structured correctly.","type":"content","url":"/esg-voc#flexibility-for-other-repositories","position":19},{"hierarchy":{"lvl1":"ESG Voc","lvl3":"Requirements","lvl2":"Introduction"},"type":"lvl3","url":"/esg-voc#requirements","position":20},{"hierarchy":{"lvl1":"ESG Voc","lvl3":"Requirements","lvl2":"Introduction"},"content":"Python Version: 3.12 or higher.\n\nNo additional system dependencies: interaction with the library is entirely Python-based, with no other external dependencies like SQLite.\n\nThis introduction covers the general purpose and installation of ESGVOC. In the next sections, we will dive deeper into its functionality, including the Python API and CLI usage.","type":"content","url":"/esg-voc#requirements","position":21},{"hierarchy":{"lvl1":"ESG Voc","lvl3":"Helpful Links:","lvl2":"Introduction"},"type":"lvl3","url":"/esg-voc#helpful-links","position":22},{"hierarchy":{"lvl1":"ESG Voc","lvl3":"Helpful Links:","lvl2":"Introduction"},"content":"For more information check these links:\n\nESG Voc Documentation: \n\nESG Voc Intro\n\nGithub: \n\nESG Voc Github\n\nControlled Vocabulary: \n\nCV‚Äôs\n\nCMIP_CV's Github Repository: \n\nWCRP-CMIP","type":"content","url":"/esg-voc#helpful-links","position":23},{"hierarchy":{"lvl1":"ESGF-Compute Guide"},"type":"lvl1","url":"/esgf-compute","position":0},{"hierarchy":{"lvl1":"ESGF-Compute Guide"},"content":"","type":"content","url":"/esgf-compute","position":1},{"hierarchy":{"lvl1":"ESGF-Compute Guide"},"type":"lvl1","url":"/esgf-compute#esgf-compute-guide","position":2},{"hierarchy":{"lvl1":"ESGF-Compute Guide"},"content":"","type":"content","url":"/esgf-compute#esgf-compute-guide","position":3},{"hierarchy":{"lvl1":"ESGF Docker Guide"},"type":"lvl1","url":"/esgf-docker","position":0},{"hierarchy":{"lvl1":"ESGF Docker Guide"},"content":"","type":"content","url":"/esgf-docker","position":1},{"hierarchy":{"lvl1":"ESGF Docker Guide","lvl2":"esgf-docker"},"type":"lvl2","url":"/esgf-docker#esgf-docker","position":2},{"hierarchy":{"lvl1":"ESGF Docker Guide","lvl2":"esgf-docker"},"content":"This repository contains the Dockerfiles and associated deployment artifacts for building\nand running the ESGF stack as Docker images.\n\nImages are built automatically for every commit that modifies the images directory and pushed\nto Docker Hub under the \n\nesgfdeploy organisation.\n\nThe ESGF stack can be deployed in one of two ways:\n\nUsing Ansible to deploy and configure containers on specific hosts\n\nUsing Helm to deploy containers to a Kubernetes cluster\n\nThe Kubernetes deployment is recommended if possible, but we recognise that not all sites will\nbe comfortable configuring and maintaining a Kubernetes cluster. However Ansible-based deployments\nwill not benefit from many features provided by Kubernetes, including:\n\nZero downtime upgrades\n\nHealth checks providing increased resilience\n\nAutomatic scaling and load-balancing\n\nAggregated logging and metrics","type":"content","url":"/esgf-docker#esgf-docker","position":3},{"hierarchy":{"lvl1":"ESGF Docker Guide","lvl3":"Current status","lvl2":"esgf-docker"},"type":"lvl3","url":"/esgf-docker#current-status","position":4},{"hierarchy":{"lvl1":"ESGF Docker Guide","lvl3":"Current status","lvl2":"esgf-docker"},"content":"This project is under heavy active development, with the implementation depending on the ESGF\nFuture Architecture discussions.\n\nCurrently, data and index nodes are implemented but without authentication.\n\nThe data node uses THREDDS to serve catalog and OPeNDAP endpoints, but uses Nginx for direct file\nserving which should be more performant than THREDDS.\n\nThe data node is capable of using existing catalogs from the current publisher to specify the\navailable data, however it is designed primarily to use a catalog-free configuration which utilises\n\n\ndatasetScan elements,\nto serve all files under a given dataset root. This will work with the next-generation publisher\nbeing developed at LLNL that does not rely on THREDDS catalogs for publishing metadata.","type":"content","url":"/esgf-docker#current-status","position":5},{"hierarchy":{"lvl1":"ESGF Docker Guide","lvl3":"Image tags","lvl2":"esgf-docker"},"type":"lvl3","url":"/esgf-docker#image-tags","position":6},{"hierarchy":{"lvl1":"ESGF Docker Guide","lvl3":"Image tags","lvl2":"esgf-docker"},"content":"Each image that is built for ESGF Docker is given several tags. Some of these are immutable, which\nmeans they refer to a fixed version of the image for all time, and some are mutable which means\nthat the underlying image will change over time.\n\nESGF Docker will apply the following tags when building images:\n\nMutable tags\n\nlatest: the latest build for the master branch\n\n<slugified-branch-name>: the latest build for the given branch name, as a slug, e.g.\nfor the branch issue/112/nginx-data-node use issue-112-nginx-data-node\n\nImmutable tags\n\nThe short Git hash for the commit that triggered the build, e.g. d65ca162, a031a2ca\n\nThe tag name for any tagged releases\n\nBy default, both the Ansible and Kubernetes installations use the latest tag when specifying\nDocker images, which is a mutable tag.\n\nFor production installations it is recommended to use an immutable tag, either for a tagged\nrelease or a particular commit, in order to avoid unexpected code changes or differences in\nthe container image between load-balanced nodes.\n\nYou can check the \n\navailable tags on Docker Hub.\nAll the ESGF Docker images are built together, so any given tag will be available for all images.","type":"content","url":"/esgf-docker#image-tags","position":7},{"hierarchy":{"lvl1":"ESGF Docker Guide","lvl3":"Making a deployment","lvl2":"esgf-docker"},"type":"lvl3","url":"/esgf-docker#making-a-deployment","position":8},{"hierarchy":{"lvl1":"ESGF Docker Guide","lvl3":"Making a deployment","lvl2":"esgf-docker"},"content":"Whether deploying ESGF using Kubernetes or Ansible, the first step is to clone the repository:git clone https://github.com/ESGF/esgf-docker.git\ncd esgf-docker\n\nThen follow the deployment guide for your chosen deployment method:\n\nDeploy ESGF using Ansible\n\nDeploy ESGF to Kubernetes using Helm","type":"content","url":"/esgf-docker#making-a-deployment","position":9},{"hierarchy":{"lvl1":"ESGF Installer"},"type":"lvl1","url":"/esgf-installer","position":0},{"hierarchy":{"lvl1":"ESGF Installer"},"content":"","type":"content","url":"/esgf-installer","position":1},{"hierarchy":{"lvl1":"ESGF Installer","lvl2":"New and returning installations"},"type":"lvl2","url":"/esgf-installer#new-and-returning-installations","position":2},{"hierarchy":{"lvl1":"ESGF Installer","lvl2":"New and returning installations"},"content":"Regardless of whether you have installed and administered an ESGF node previously, please read the following document on ESGF policies, as this should influence what type on installation you should do:\n\nhttp://‚Äãesgf‚Äã.llnl‚Äã.gov‚Äã/media‚Äã/pdf‚Äã/ESGF‚Äã-Policies‚Äã-and‚Äã-Guidelines‚Äã-V1‚Äã.0‚Äã.pdf","type":"content","url":"/esgf-installer#new-and-returning-installations","position":3},{"hierarchy":{"lvl1":"ESGF Installer","lvl2":"ESGF Docker Installation"},"type":"lvl2","url":"/esgf-installer#esgf-docker-installation","position":4},{"hierarchy":{"lvl1":"ESGF Installer","lvl2":"ESGF Docker Installation"},"content":"ESGF has adopted the use of containers for new node installations and upgrades, please see:\n\nhttps://‚Äãgithub‚Äã.com‚Äã/esgf‚Äã/esgf‚Äã-docker\n\nSpecific instructions for deployment methods are linked at the bottom of the README.","type":"content","url":"/esgf-installer#esgf-docker-installation","position":5},{"hierarchy":{"lvl1":"ESGF Installer","lvl2":"Installation"},"type":"lvl2","url":"/esgf-installer#installation","position":6},{"hierarchy":{"lvl1":"ESGF Installer","lvl2":"Installation"},"content":"To setup a ‚Äòdevel‚Äô installcd /usr/local/bin\nwget -O esg-bootstrap http://distrib-coffee.ipsl.jussieu.fr/pub/esgf/dist/devel/esgf-installer/2.5/esg-bootstrap --no-check-certificate  \nchmod 555 esg-bootstrap  \n./esg-bootstrap --devel   \n\nTo setup a ‚Äòmaster‚Äô installwget -O esg-bootstrap http://distrib-coffee.ipsl.jussieu.fr/pub/esgf/dist/esgf-installer/2.5/esg-bootstrap --no-check-certificate  \nchmod 555 esg-bootstrap  \n./esg-bootstrap\n\nMore detailed installation instructions can be found on the \n\nwiki","type":"content","url":"/esgf-installer#installation","position":7},{"hierarchy":{"lvl1":"ESGF Installer","lvl2":"Support"},"type":"lvl2","url":"/esgf-installer#support","position":8},{"hierarchy":{"lvl1":"ESGF Installer","lvl2":"Support"},"content":"Please \n\nopen an issue for support.\nPlease follow the \n\nIssue Tracking Guidelines when opening a new issue.","type":"content","url":"/esgf-installer#support","position":9},{"hierarchy":{"lvl1":"ESGF Installer","lvl2":"Contributing"},"type":"lvl2","url":"/esgf-installer#contributing","position":10},{"hierarchy":{"lvl1":"ESGF Installer","lvl2":"Contributing"},"content":"Please contribute using \n\nGithub Flow. Create a branch, add commits, and \n\nopen a pull request.","type":"content","url":"/esgf-installer#contributing","position":11},{"hierarchy":{"lvl1":"Intake-ESGF"},"type":"lvl1","url":"/intake-esgf-1","position":0},{"hierarchy":{"lvl1":"Intake-ESGF"},"content":"","type":"content","url":"/intake-esgf-1","position":1},{"hierarchy":{"lvl1":"Intake-ESGF","lvl2":"Overview"},"type":"lvl2","url":"/intake-esgf-1#overview","position":2},{"hierarchy":{"lvl1":"Intake-ESGF","lvl2":"Overview"},"content":"intake-esgf is an intake and intake-esm inspired package under\ndevelopment in ESGF2. The data catalog is populated by querying a number of\nindex nodes and puts together a global view of where the datasets may be found.\nIf you are familiar with the interface for intake-esm, then using this\npackage should be straightforward.","type":"content","url":"/intake-esgf-1#overview","position":3},{"hierarchy":{"lvl1":"Intake-ESGF","lvl2":"Helpful Links"},"type":"lvl2","url":"/intake-esgf-1#helpful-links","position":4},{"hierarchy":{"lvl1":"Intake-ESGF","lvl2":"Helpful Links"},"content":"intake-esgf: \n\nIntake-ESGF\n\nBeginner's Guide to ESGF: \n\nBeginner‚Äôs Guide\n\nGithub Page: \n\nIntake-ESGF Github","type":"content","url":"/intake-esgf-1#helpful-links","position":5},{"hierarchy":{"lvl1":"Intake-ESGF"},"type":"lvl1","url":"/intake-esgf-1","position":0},{"hierarchy":{"lvl1":"Intake-ESGF"},"content":"","type":"content","url":"/intake-esgf-1","position":1},{"hierarchy":{"lvl1":"Intake-ESGF","lvl2":"Overview"},"type":"lvl2","url":"/intake-esgf-1#overview","position":2},{"hierarchy":{"lvl1":"Intake-ESGF","lvl2":"Overview"},"content":"intake-esgf is an intake and intake-esm inspired package under\ndevelopment in ESGF2. The data catalog is populated by querying a number of\nindex nodes and puts together a global view of where the datasets may be found.\nIf you are familiar with the interface for intake-esm, then using this\npackage should be straightforward.","type":"content","url":"/intake-esgf-1#overview","position":3},{"hierarchy":{"lvl1":"Intake-ESGF","lvl2":"Helpful Links"},"type":"lvl2","url":"/intake-esgf-1#helpful-links","position":4},{"hierarchy":{"lvl1":"Intake-ESGF","lvl2":"Helpful Links"},"content":"intake-esgf: \n\nIntake-ESGF\n\nBeginner's Guide to ESGF: \n\nBeginner‚Äôs Guide\n\nGithub Page: \n\nIntake-ESGF Github","type":"content","url":"/intake-esgf-1#helpful-links","position":5},{"hierarchy":{"lvl1":"Metagrid Developer Guide"},"type":"lvl1","url":"/metagrid-developer-guide","position":0},{"hierarchy":{"lvl1":"Metagrid Developer Guide"},"content":"","type":"content","url":"/metagrid-developer-guide","position":1},{"hierarchy":{"lvl1":"Node Installation Guide"},"type":"lvl1","url":"/node-installation","position":0},{"hierarchy":{"lvl1":"Node Installation Guide"},"content":"","type":"content","url":"/node-installation","position":1},{"hierarchy":{"lvl1":"Node Installation Guide","lvl2":"Data node configuration"},"type":"lvl2","url":"/node-installation#data-node-configuration","position":2},{"hierarchy":{"lvl1":"Node Installation Guide","lvl2":"Data node configuration"},"content":"This section describes the most commonly used data node configuration options.\nFor a full list of available variables, please consult the chart at\n\n\nvalues.yaml. TOC depthFrom:2 \n\nConfiguring the available datasets\n\nFowarding access logs\n\nEnabling demand-based autoscaling\n\nUsing existing THREDDS catalogs\n\nImproving pod startup time for large catalogs /TOC ","type":"content","url":"/node-installation#data-node-configuration","position":3},{"hierarchy":{"lvl1":"Node Installation Guide","lvl3":"Configuring the available datasets","lvl2":"Data node configuration"},"type":"lvl3","url":"/node-installation#configuring-the-available-datasets","position":4},{"hierarchy":{"lvl1":"Node Installation Guide","lvl3":"Configuring the available datasets","lvl2":"Data node configuration"},"content":"By default, the data node uses a catalog-free configuration where the available data is defined simply by\na series of datasets. For each dataset, all files under the specified path will be served using both\nOPeNDAP (for NetCDF files) and plain HTTP. The browsable interface and OPeNDAP are provided by\nTHREDDS and direct file serving is provided by Nginx.\n\nThe configuration of the datasets is done using two variables:\n\ndata.mounts: List of volumes to mount into the container. Each item should contain the keys:\n\nmountPath: The path to mount the volume inside the container\n\nvolumeSpec: A \n\nKubernetes volume specification for\nthe volume containing the data\n\nname (optional): A name for the volume - by default, a name is derived from the mountPath\n\nmountOptions (optional): Options for the volume mount, e.g. mountPropagation for hostPath volumes\n\ndata.datasets: List of datasets to expose. Each item should contain the keys:\n\nname: The human-readable name of the dataset, displayed in the THREDDS UI\n\npath: The URL path part for the dataset\n\nlocation: The directory path to the root of the dataset in the container\n\nWarning\n\n\n\nWhen using hostPath volumes, the data must exist at the same path on all cluster nodes where the THREDDS\nor file server pods might be scheduled.\n\nIf your data is on a shared filesystem, just mount the filesystem on your cluster nodes as you would\nwith any other host.\n\nThese variables should be defined in your values.yaml, e.g.:data:\n  mounts:\n    # This uses a hostPath volume to mount /datacentre/archive on the host as /data in the container\n    - mountPath: /data\n      volumeSpec:\n        hostPath:\n          path: /datacentre/archive\n      mountOptions:\n        # mountPropagation is particularly important if the filesystem has automounted sub-mounts\n        mountPropagation: HostToContainer\n\n  datasets:\n    # This will expose files at /data/cmip6/[path] in the container\n    # as http://esgf-data.example.org/thredds/{dodsC,fileServer}/esg_cmip6/[path]\n    - name: CMIP6\n      path: esg_cmip6\n      location: /data/cmip6\n    # Similarly, this exposes files at /data/cordex/[path] in the container\n    # as http://esgf-data.example.org/thredds/{dodsC,fileServer}/esg_cordex/[path]\n    - name: CORDEX\n      path: esg_cordex\n      location: /data/cordex","type":"content","url":"/node-installation#configuring-the-available-datasets","position":5},{"hierarchy":{"lvl1":"Node Installation Guide","lvl3":"Using S3 buckets for data","lvl2":"Data node configuration"},"type":"lvl3","url":"/node-installation#using-s3-buckets-for-data","position":6},{"hierarchy":{"lvl1":"Node Installation Guide","lvl3":"Using S3 buckets for data","lvl2":"Data node configuration"},"content":"An S3 location for a dataset can be specified using s3Location instead of location in the yaml config, e.g.:  datasets:\n    - name: CMIP6\n      path: esg_cmip6\n      s3Location:\n        host: example.com\n        port: 443\n        bucket: bucket_name\n        dataPath: path/to/files\n\nWe don‚Äôt currently support adding security parameters for accessing secured S3 buckets.","type":"content","url":"/node-installation#using-s3-buckets-for-data","position":7},{"hierarchy":{"lvl1":"Node Installation Guide","lvl3":"Fowarding access logs","lvl2":"Data node configuration"},"type":"lvl3","url":"/node-installation#fowarding-access-logs","position":8},{"hierarchy":{"lvl1":"Node Installation Guide","lvl3":"Fowarding access logs","lvl2":"Data node configuration"},"content":"The THREDDS and Nginx file server components can be configured to forward access logs to\n\n\nCMCC for processing in order to produce download statistics for\nthe federation.\n\nBefore enabling this functionality you must first contact CMCC to arrange for the IP addresses\nof your Kubernetes nodes, as visible from the internet, to be whitelisted.\n\nIf your Kubernetes nodes are not directly exposed to the internet then they are probably using\n\n\nNetwork Address Translation (NAT)\nwhen accessing resources on the internet.\n\nIn this case, the address that you need to give to CMCC is the translated address.\n\nTo enable the forwarding of access logs for THREDDS and Nginx file server pods, add the following\nto your values.yaml:data:\n  accessLogSidecar:\n    enabled: true\n\nAdditional variables are available to configure the server to which logs should be forwarded,\nhowever the vast majority of deployments will not need to change these.","type":"content","url":"/node-installation#fowarding-access-logs","position":9},{"hierarchy":{"lvl1":"Node Installation Guide","lvl3":"Enabling demand-based autoscaling","lvl2":"Data node configuration"},"type":"lvl3","url":"/node-installation#enabling-demand-based-autoscaling","position":10},{"hierarchy":{"lvl1":"Node Installation Guide","lvl3":"Enabling demand-based autoscaling","lvl2":"Data node configuration"},"content":"Kubernetes allows the number of pods backing a service to be scaled up and down automatically using\na \n\nHorizontal Pod Autoscaler (HPA).\nThis allows the service to respond to spikes in demand by creating more pods to respond to requests.\nA Kubernetes Service ensures that requests are routed to the new replicas as they become ready.\n\nA HPA can be configured to automatically adjust the number of replicas based on any metrics that are exposed via\nthe \n\nMetrics API.\nBy default, this allows scaling based on the CPU or memory usage of the pods backing a service. However\nit is possible to integrate other metrics gathering systems, such as \n\nPrometheus,\nto allow scaling based on any of the collected metrics (e.g. network I/O, requests per second).\n\nBy default, autoscaling is disabled in the ESGF Helm chart. To enable autoscaling for the THREDDS and\nNginx file server components, the chart allows HorizontalPodAutoscaler resources to be defined using\nthe data.{thredds,fileServer}.hpa variables. These variables define the spec section of the HPA, except\nfor the scaleTargetRef section which is automatically populated with the correct reference.\nFor more information about HPA configuration, see the\n\n\nKubernetes HPA Walkthrough.\n\nWarning\n\n\n\nIn order to scale based on utilisation (as opposed to absolute value), you must define\nresources.requests for the service\n(see \n\nConfiguring container resources above).\n\nFor example, the following configuration would attempt to keep the average CPU utilisation\nbelow 80% of the requested amount by scaling out up to a maximum of 10 replicas:data:\n  thredds:\n    hpa:\n      minReplicas: 1\n      maxReplicas: 10\n      metrics:\n        - type: Resource\n          resource:\n            name: cpu\n            target:\n              type: Utilization\n              averageUtilization: 80\n\n  fileServer:\n    hpa:\n      minReplicas: 1\n      maxReplicas: 10\n      metrics:\n        - type: Resource\n          resource:\n            name: cpu\n            target:\n              type: Utilization\n              averageUtilization: 80","type":"content","url":"/node-installation#enabling-demand-based-autoscaling","position":11},{"hierarchy":{"lvl1":"Node Installation Guide","lvl3":"Using existing THREDDS catalogs","lvl2":"Data node configuration"},"type":"lvl3","url":"/node-installation#using-existing-thredds-catalogs","position":12},{"hierarchy":{"lvl1":"Node Installation Guide","lvl3":"Using existing THREDDS catalogs","lvl2":"Data node configuration"},"content":"The data node can be configured to serve data based on pre-existing THREDDS catalogs, for\nexample those generated by the ESGF publisher. To do this, you must specify the volume\ncontaining the catalogs using the variable data.thredds.catalogVolume. This volume must\nbe available to all nodes where THREDDS pods might be scheduled and must be able to be\nmounted in multiple pods at once, for example a hostPath using a shared filesystem.\nThis variable should contain the keys volumeSpec and mountOptions, which have the\nsame meaning as for data.mounts above, e.g.:data:\n  thredds:\n    catalogVolume:\n      volumeSpec:\n        hostPath:\n          path: /path/to/shared/catalogs\n      mountOptions:\n        mountPropagation: HostToContainer\n\nNote\n\n\n\nYou must still configure data.mounts and data.datasets as above, except in this case the\ndatasets should correspond the to the datasetRoots in your THREDDS catalogs.\n\nWhen the catalogs change, run the Helm chart in order to create new pods which will\nload the new catalogs. This will be done using a rolling upgrade with no downtime - the\nold pods will continue to serve requests with the old catalogs until new pods are ready.\n\nFor large catalogs, you may also need to adjust the startup time for the THREDDS container\nas THREDDS must build the catalog cache before it can start serving requests. To do this,\nspecify data.thredds.startTimeout, which specifies the number of seconds to wait for\nTHREDDS to start before assuming there is a problem and trying again (default 300):data:\n  thredds:\n    startTimeout: 3600  # Large catalogs may take an hour or more","type":"content","url":"/node-installation#using-existing-thredds-catalogs","position":13},{"hierarchy":{"lvl1":"Node Installation Guide","lvl3":"Improving pod startup time for large catalogs","lvl2":"Data node configuration"},"type":"lvl3","url":"/node-installation#improving-pod-startup-time-for-large-catalogs","position":14},{"hierarchy":{"lvl1":"Node Installation Guide","lvl3":"Improving pod startup time for large catalogs","lvl2":"Data node configuration"},"content":"Pods in Kubernetes are ephemeral, meaning they do not preserve state across restarts.\nThis includes the THREDDS caches, meaning that every time a pod starts it will spend time\nrebuilding the catalog cache before serving requests, even if the catalogs have not changed.\nThis is exacerbated by the fact that the catalogs will likely be on network-attached-storage\nin order to facilitate sharing across nodes, meaning higher latency for stat and read\noperations.\n\nFor large catalogs, this can result in THREDDS pods taking an hour or more to start. This is not\nmerely an inconvenience - in order to benefit from advanced features in Kubernetes such as\nrecovery from failure and demand-based autoscaling, pods must start quickly in order to begin\ntaking load as soon as possible. There are two things that can be done to address this problem:\n\nKeep a copy of the catalogs on the local disk of each node that may have THREDDS pods scheduled\n\nPre-build the catalog cache (again on the local disk of each node) and use it to seed the cache for new THREDDS pods\n\nIn an ESGF deployment, this is acheived by having a\n\n\nDaemonSet that runs on\neach node. When the Helm chart is run or a new node is added to the cluster, this DaemonSet\nwill syncronise the THREDDS catalogs to each node‚Äôs local disk and run THREDDS to build the catalog\ncache. The THREDDS pods will wait for the DaemonSet to finish updating the cache before starting,\nusing the pre-built cache as a seed for their own local caches. While they are waiting, the old\npods will continue to serve requests using the old catalogs, so the upgrade is zero-downtime.\nUsing this approach, copying the catalogs to local disk and rebuilding the cache are one-time\noperations and the THREDDS pods start much faster (less than one minute for a large catalog at\nCEDA in testing).\n\nTo enable local caching of catalogs for a deployment, just set data.thredds.localCache.enabled:data:\n  thredds:\n    localCache:\n      enabled: true","type":"content","url":"/node-installation#improving-pod-startup-time-for-large-catalogs","position":15},{"hierarchy":{"lvl1":"ESGF2-US"},"type":"lvl1","url":"/","position":0},{"hierarchy":{"lvl1":"ESGF2-US"},"content":"\n\nWelcome to our the Earth System Grid Federation 2 - US (ESGF2-US) Project!\n\nI am looking for data üîé\n\nCheck out \n\nmetagrid, our search interface for finding ESGF Data!\n\nWhat is ESGF? üåê\n\nCheck out the \n\nESGF2-US Overview.\n\nI need examples üíª\n\nCheck out our \n\nESGF Computational Cookbook","type":"content","url":"/","position":1},{"hierarchy":{"lvl1":"Back End?"},"type":"lvl1","url":"/backend","position":0},{"hierarchy":{"lvl1":"Back End?"},"content":"","type":"content","url":"/backend","position":1},{"hierarchy":{"lvl1":"Back End?"},"type":"lvl1","url":"/backend#back-end","position":2},{"hierarchy":{"lvl1":"Back End?"},"content":"","type":"content","url":"/backend#back-end","position":3},{"hierarchy":{"lvl1":"Data Access"},"type":"lvl1","url":"/data-users","position":0},{"hierarchy":{"lvl1":"Data Access"},"content":"There are a lot of ways a user can access data:\n\nMetagrid: MetaGrid is the next-generation search interface for the Earth System Grid Federation (ESGF). It‚Äôs a user-friendly web application designed to help researchers find and access climate model data stored within the ESGF. Think of it as a more advanced way to search for and retrieve the data needed for climate research.\nFor more information visit \n\nMetagrid\n\nIntake-ESGF: intake-esgf is an \n\nintake-esm inspired package under development in ESGF2. The main difference is that in place of querying a static index which is completely loaded at runtime, intake-esgf catalogs initialize empty and are populated by searching, querying ESGF index nodes.For more information visit \n\nIntake-ESGF\n\nCookbooks: For more information visit \n\nCookbooks\n\nESG Pull: For more information visit \n\nESG Pull","type":"content","url":"/data-users","position":1},{"hierarchy":{"lvl1":"Kubernetes Guide"},"type":"lvl1","url":"/kubernetes","position":0},{"hierarchy":{"lvl1":"Kubernetes Guide"},"content":"","type":"content","url":"/kubernetes","position":1},{"hierarchy":{"lvl1":"Kubernetes Guide","lvl2":"Deploy ESGF using Kubernetes"},"type":"lvl2","url":"/kubernetes#deploy-esgf-using-kubernetes","position":2},{"hierarchy":{"lvl1":"Kubernetes Guide","lvl2":"Deploy ESGF using Kubernetes"},"content":"This project provides a \n\nHelm chart to deploy ESGF resources\non a \n\nKubernetes cluster.\n\nThe chart is in \n\ndeploy‚Äã/kubernetes‚Äã/chart. Please look at the\nfiles to understand exactly what resources are being created.\n\nFor a complete list of all the variables that are available, please look at the\n\n\nvalues.yaml for the chart. The defaults there have\nextensive comments that explain how to use these variables. This documentation describes how to\napply some common configurations. TOC depthFrom:2 \n\nInstalling/upgrading ESGF\n\nLocal test installation with Minikube\n\nConfiguring the installation /TOC ","type":"content","url":"/kubernetes#deploy-esgf-using-kubernetes","position":3},{"hierarchy":{"lvl1":"Kubernetes Guide","lvl3":"Installing/upgrading ESGF","lvl2":"Deploy ESGF using Kubernetes"},"type":"lvl3","url":"/kubernetes#installing-upgrading-esgf","position":4},{"hierarchy":{"lvl1":"Kubernetes Guide","lvl3":"Installing/upgrading ESGF","lvl2":"Deploy ESGF using Kubernetes"},"content":"Before attempting to install the ESGF Helm chart, you must have the following:\n\nA Kubernetes cluster with an\n\n\nIngress Controller enabled\n\nkubectl installed and configured to talk\nto your cluster\n\nHelm installed\n\nNext, make a configuration directory - this can be anywhere on your machine that is not under\nesgf-docker. You can also place this directory under version control if you wish - this can be very\nuseful for tracking changes to the configuration, or even triggering deployments automatically when\nconfiguration changes.\n\nIn your configuration directory, make a new YAML file called values.yaml and override any variables to fit\nyour deployment. The only required variable is hostname, which should be the DNS name at which your\nESGF deployment will be available:hostname: esgf.example.org\n\nNote\n\n\n\nThe Helm chart does not create a DNS entry for the hostname. This must be separately configured\nto point to the ingress controller for your Kubernetes cluster.\n\nOnce you have configured your values.yaml, you can install or upgrade ESGF using the Helm chart. If no\nnamespace is specified, it will use the default namespace for your kubectl configuration:helm upgrade -i [-n <namespace>] -f /my/esgf/config/values.yaml --wait esgf ./deploy/kubernetes/chart","type":"content","url":"/kubernetes#installing-upgrading-esgf","position":5},{"hierarchy":{"lvl1":"Kubernetes Guide","lvl3":"Local test installation with Minikube","lvl2":"Deploy ESGF using Kubernetes"},"type":"lvl3","url":"/kubernetes#local-test-installation-with-minikube","position":6},{"hierarchy":{"lvl1":"Kubernetes Guide","lvl3":"Local test installation with Minikube","lvl2":"Deploy ESGF using Kubernetes"},"content":"For local test deployments, you can use \n\nMinikube\nwith data from \n\nroocs‚Äã/mini‚Äã-esgf‚Äã-data:# Start the minikube cluster\nminikube start\n# Enable the ingress addon\nminikube addons enable ingress\n# Install the test data\nminikube ssh \"curl -fsSL https://github.com/roocs/mini-esgf-data/tarball/master | sudo tar -xz --strip-components=1 -C / --wildcards */test_data\"\n\nConfigure the chart to serve the test data (see\n\n\nminikube‚Äã-values‚Äã.yaml), using a nip.io\ndomain pointing to the Minikube server:helm install esgf ./deploy/kubernetes/chart/ \\\n  -f ./deploy/kubernetes/minikube-values.yaml \\\n  --set hostname=\"$(minikube ip).nip.io\"\n\nOnce the containers have started, the THREDDS interface will be available at http://$(minikube ip).nip.io/thredds.","type":"content","url":"/kubernetes#local-test-installation-with-minikube","position":7},{"hierarchy":{"lvl1":"Kubernetes Guide","lvl3":"Configuring the installation","lvl2":"Deploy ESGF using Kubernetes"},"type":"lvl3","url":"/kubernetes#configuring-the-installation","position":8},{"hierarchy":{"lvl1":"Kubernetes Guide","lvl3":"Configuring the installation","lvl2":"Deploy ESGF using Kubernetes"},"content":"See \n\nConfiguring a Kubernetes deployment.","type":"content","url":"/kubernetes#configuring-the-installation","position":9},{"hierarchy":{"lvl1":"Metagrid Guide"},"type":"lvl1","url":"/metagrid-guide","position":0},{"hierarchy":{"lvl1":"Metagrid Guide"},"content":"","type":"content","url":"/metagrid-guide","position":1},{"hierarchy":{"lvl1":"Metagrid Guide","lvl2":"Learn more about Datasets"},"type":"lvl2","url":"/metagrid-guide#learn-more-about-datasets","position":2},{"hierarchy":{"lvl1":"Metagrid Guide","lvl2":"Learn more about Datasets"},"content":"Well we all have a basic Idea of what ESGF is, if not please look at this  (https://intake-esgf.readthedocs.io/en/latest/)` for a better understanding.\n\nLets start with every possible question that a newbie like me would have.","type":"content","url":"/metagrid-guide#learn-more-about-datasets","position":3},{"hierarchy":{"lvl1":"Metagrid Guide","lvl3":"What is Metagrid?","lvl2":"Learn more about Datasets"},"type":"lvl3","url":"/metagrid-guide#what-is-metagrid","position":4},{"hierarchy":{"lvl1":"Metagrid Guide","lvl3":"What is Metagrid?","lvl2":"Learn more about Datasets"},"content":"MetaGrid is the next-generation search interface for the Earth System Grid Federation (ESGF). It‚Äôs a user-friendly web application designed to help researchers find and access climate model data stored within the ESGF. Think of it as a more advanced way to search for and retrieve the data needed for climate research.\n\nHere‚Äôs a more detailed breakdown:\n\nESGF‚Äôs Role:\nThe Earth System Grid Federation (ESGF) is a global network of data centers that provides access to climate model outputs. It‚Äôs a crucial resource for climate scientists, particularly for projects like the Intergovernmental Panel on Climate Change (IPCC) assessments.","type":"content","url":"/metagrid-guide#what-is-metagrid","position":5},{"hierarchy":{"lvl1":"Metagrid Guide","lvl3":"MetaGrid‚Äôs Function:","lvl2":"Learn more about Datasets"},"type":"lvl3","url":"/metagrid-guide#metagrids-function","position":6},{"hierarchy":{"lvl1":"Metagrid Guide","lvl3":"MetaGrid‚Äôs Function:","lvl2":"Learn more about Datasets"},"content":"MetaGrid is the tool that users interact with to search through the vast amount of data available through ESGF. It offers features like:\n\nFacet Value Free-Text Entry: Allows users to quickly find relevant options by typing instead of scrolling through long lists of facets (like model names, variables, etc.).\n\nSaved Searches: Users can save their search queries and reuse them later.\nShareable Result Links: Enables users to share specific search results with colleagues.\n\nHow it helps:\nMetaGrid simplifies the process of finding the specific climate data needed for research, making it easier to access and utilize the data available through ESGF.\n\nHere‚Äôs the link to Metagrid https://aims2.llnl.gov/","type":"content","url":"/metagrid-guide#metagrids-function","position":7},{"hierarchy":{"lvl1":"Metagrid Guide","lvl3":"How do I use this interface?","lvl2":"Learn more about Datasets"},"type":"lvl3","url":"/metagrid-guide#how-do-i-use-this-interface","position":8},{"hierarchy":{"lvl1":"Metagrid Guide","lvl3":"How do I use this interface?","lvl2":"Learn more about Datasets"},"content":"","type":"content","url":"/metagrid-guide#how-do-i-use-this-interface","position":9},{"hierarchy":{"lvl1":"Replicators"},"type":"lvl1","url":"/replicator","position":0},{"hierarchy":{"lvl1":"Replicators"},"content":"","type":"content","url":"/replicator","position":1},{"hierarchy":{"lvl1":"Replicators","lvl2":"ESGF-NG Replicator"},"type":"lvl2","url":"/replicator#esgf-ng-replicator","position":2},{"hierarchy":{"lvl1":"Replicators","lvl2":"ESGF-NG Replicator"},"content":"The ESGF-NG Replicator is a service designed to be used by ESGF Data Node operators with the new \n\nESGF-NG core architecture.\n\nIt monitors the ESGF-wide Event Stream.\n\nWhen it sees a new ESGF dataset that matches a configured replication policy, it makes a replica of the dataset in configured local storage.\n\nIt updates the configured ESGF STAC Catalogue with information about any replicas it creates.\n\nThe design-docs folder contains the following:\n\nUser Stories\n\nRequirements\n\nKey Design Points, aka Technical Requirements\n\nOpen Questions","type":"content","url":"/replicator#esgf-ng-replicator","position":3}]}